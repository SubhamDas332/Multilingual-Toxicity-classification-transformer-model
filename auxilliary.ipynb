{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a2ccebc-0472-425c-b156-56eb0b607fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved extracted texts from train & dev to tokenizer_corpus.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Input and output file paths\n",
    "train_file = \"train.tsv\"  # Replace with actual path\n",
    "dev_file = \"dev.tsv\"      # Replace with actual path\n",
    "output_file = \"tokenizer_corpus.txt\"\n",
    "\n",
    "# Read TSV files\n",
    "df_train = pd.read_csv(train_file, sep=\"\\t\")\n",
    "df_dev = pd.read_csv(dev_file, sep=\"\\t\")\n",
    "\n",
    "# Extract only the 'text' column and combine them\n",
    "all_texts = pd.concat([df_train[\"text\"], df_dev[\"text\"]])\n",
    "\n",
    "# Save to a single file (one sentence per line)\n",
    "all_texts.to_csv(output_file, index=False, header=False, quoting=3, escapechar=\"\\\\\")  # Escape special chars\n",
    "\n",
    "print(f\"Saved extracted texts from train & dev to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6cadfab-0bbf-4703-b7da-deb5f839131b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created corpus with language tokens in language_tokenizer_corpus.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "### LANGUAGE TOKEN INCLUDED\n",
    "def extract_language_code(id_str):\n",
    "    \"\"\"Extract language code from ID (first part before underscore)\"\"\"\n",
    "    return id_str.split('_')[0]\n",
    "\n",
    "def process_text(text, lang_code):\n",
    "    return f\"[LANG={lang_code}] {text}\"  \n",
    "\n",
    "def create_corpus(input_files, output_file):\n",
    "    \"\"\"Process TSV files and create training corpus\"\"\"\n",
    "    corpus = []\n",
    "    \n",
    "    for file in input_files:\n",
    "        df = pd.read_csv(file, sep='\\t')\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            lang_code = extract_language_code(row['id'])\n",
    "            processed_text = process_text(row['text'], lang_code)\n",
    "            \n",
    "            # Add to corpus if text is not empty\n",
    "            if processed_text.strip():\n",
    "                corpus.append(processed_text)\n",
    "    \n",
    "    # Write to output file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(corpus))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    INPUT_FILES = ['train.tsv', 'dev.tsv']\n",
    "    OUTPUT_FILE = 'language_tokenizer_corpus.txt'\n",
    "    \n",
    "    # Create the corpus\n",
    "    create_corpus(INPUT_FILES, OUTPUT_FILE)\n",
    "    print(f\"Created corpus with language tokens in {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2760eba1-602f-4d25-90ea-b0bbe86894d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input: '[LANG=eng] Hello world'\n",
      "Token IDs: [5, 9513, 806]\n",
      "Tokens: ['[LANG=eng]', 'ĠHello', 'Ġworld']\n",
      "Decoded Text: '[LANG=eng] ĠHello Ġworld'\n",
      "Simulated Input IDs with [CLS] and [SEP]: [1, 5, 9513, 806, 2]\n",
      "Simulated Decoded Input: '[CLS] [LANG=eng] ĠHello Ġworld [SEP]'\n",
      "\n",
      "Input: '[LANG=fin] Moi maailma'\n",
      "Token IDs: [6, 5078, 80, 447, 612, 3556]\n",
      "Tokens: ['[LANG=fin]', 'ĠMo', 'i', 'Ġma', 'ail', 'ma']\n",
      "Decoded Text: '[LANG=fin] ĠMo i Ġma ail ma'\n",
      "Simulated Input IDs with [CLS] and [SEP]: [1, 6, 5078, 80, 447, 612, 3556, 2]\n",
      "Simulated Decoded Input: '[CLS] [LANG=fin] ĠMo i Ġma ail ma [SEP]'\n",
      "\n",
      "Input: '[LANG=ger] Hallo Welt'\n",
      "Token IDs: [7, 6642, 86, 15974]\n",
      "Tokens: ['[LANG=ger]', 'ĠHall', 'o', 'ĠWelt']\n",
      "Decoded Text: '[LANG=ger] ĠHall o ĠWelt'\n",
      "Simulated Input IDs with [CLS] and [SEP]: [1, 7, 6642, 86, 15974, 2]\n",
      "Simulated Decoded Input: '[CLS] [LANG=ger] ĠHall o ĠWelt [SEP]'\n",
      "\n",
      "Input: 'Hello world'\n",
      "Token IDs: [9513, 806]\n",
      "Tokens: ['ĠHello', 'Ġworld']\n",
      "Decoded Text: 'ĠHello Ġworld'\n",
      "Simulated Input IDs with [CLS] and [SEP]: [1, 9513, 806, 2]\n",
      "Simulated Decoded Input: '[CLS] ĠHello Ġworld [SEP]'\n",
      "\n",
      "Input: '[LANG=eng] A 17% cut in SSD prices'\n",
      "Token IDs: [5, 275, 4510, 12, 1326, 240, 7844, 43, 2514]\n",
      "Tokens: ['[LANG=eng]', 'ĠA', 'Ġ17', '%', 'Ġcut', 'Ġin', 'ĠSS', 'D', 'Ġprices']\n",
      "Decoded Text: '[LANG=eng] ĠA Ġ17 % Ġcut Ġin ĠSS D Ġprices'\n",
      "Simulated Input IDs with [CLS] and [SEP]: [1, 5, 275, 4510, 12, 1326, 240, 7844, 43, 2514, 2]\n",
      "Simulated Decoded Input: '[CLS] [LANG=eng] ĠA Ġ17 % Ġcut Ġin ĠSS D Ġprices [SEP]'\n",
      "\n",
      "Input: 'eng Hello world'\n",
      "Token IDs: [1728, 9513, 806]\n",
      "Tokens: ['Ġeng', 'ĠHello', 'Ġworld']\n",
      "Decoded Text: 'Ġeng ĠHello Ġworld'\n",
      "Simulated Input IDs with [CLS] and [SEP]: [1, 1728, 9513, 806, 2]\n",
      "Simulated Decoded Input: '[CLS] Ġeng ĠHello Ġworld [SEP]'\n",
      "\n",
      "Input: '[LANG=invalid] Test'\n",
      "Token IDs: [231, 235, 36, 201, 2658, 260, 68, 17400]\n",
      "Tokens: ['Ġ[', 'LANG', '=', 'in', 'val', 'id', ']', 'ĠTest']\n",
      "Decoded Text: 'Ġ[ LANG = in val id ] ĠTest'\n",
      "Simulated Input IDs with [CLS] and [SEP]: [1, 231, 235, 36, 201, 2658, 260, 68, 17400, 2]\n",
      "Simulated Decoded Input: '[CLS] Ġ[ LANG = in val id ] ĠTest [SEP]'\n",
      "\n",
      "Input: ''\n",
      "Token IDs: []\n",
      "Tokens: []\n",
      "Decoded Text: ''\n",
      "Simulated Input IDs with [CLS] and [SEP]: [1, 2]\n",
      "Simulated Decoded Input: '[CLS] [SEP]'\n",
      "\n",
      "Input: 'This is an unknownword123'\n",
      "Token IDs: [597, 250, 222, 11222, 9386, 21725]\n",
      "Tokens: ['ĠThis', 'Ġis', 'Ġan', 'Ġunknown', 'word', '123']\n",
      "Decoded Text: 'ĠThis Ġis Ġan Ġunknown word 123'\n",
      "\n",
      "Verifying language token IDs:\n",
      "ID for [LANG=eng]: 5\n",
      "ID for [LANG=fin]: 6\n",
      "ID for [LANG=ger]: 7\n",
      "ID for 'eng' (subword): 233\n"
     ]
    }
   ],
   "source": [
    "###TESTING TOKENIZER\n",
    "from tokenizers import Tokenizer\n",
    "import torch\n",
    "\n",
    "# Load the tokenizer from the JSON file\n",
    "tokenizer = Tokenizer.from_file(\"lang_bpe_tokenizer.json\")\n",
    "\n",
    "# Define sample test cases\n",
    "test_cases = [\n",
    "    \"[LANG=eng] Hello world\",\n",
    "    \"[LANG=fin] Moi maailma\",\n",
    "    \"[LANG=ger] Hallo Welt\",\n",
    "    \"Hello world\",  # No language token\n",
    "    \"[LANG=eng] A 17% cut in SSD prices\",  # Complex sentence\n",
    "    \"eng Hello world\",  # \"eng\" as a standalone word\n",
    "    \"[LANG=invalid] Test\",  # Invalid language token\n",
    "    \"\",  # Empty string\n",
    "]\n",
    "\n",
    "# Function to test and display tokenization results\n",
    "def test_tokenization(text):\n",
    "    print(f\"\\nInput: '{text}'\")\n",
    "    \n",
    "    # Encode the text to get token IDs\n",
    "    encoding = tokenizer.encode(text)\n",
    "    token_ids = encoding.ids\n",
    "    tokens = encoding.tokens\n",
    "    \n",
    "    # Decode back to text\n",
    "    decoded_text = tokenizer.decode(token_ids, skip_special_tokens=False)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Token IDs: {token_ids}\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"Decoded Text: '{decoded_text}'\")\n",
    "    \n",
    "    # Add [CLS] and [SEP] for model input simulation\n",
    "    cls_id = tokenizer.token_to_id(\"[CLS]\")\n",
    "    sep_id = tokenizer.token_to_id(\"[SEP]\")\n",
    "    input_ids = [cls_id] + token_ids + [sep_id]\n",
    "    print(f\"Simulated Input IDs with [CLS] and [SEP]: {input_ids}\")\n",
    "    print(f\"Simulated Decoded Input: '{tokenizer.decode(input_ids, skip_special_tokens=False)}'\")\n",
    "\n",
    "# Run tests for each sample\n",
    "for test_case in test_cases:\n",
    "    test_tokenization(test_case)\n",
    "\n",
    "# Additional test: Check if tokenizer handles unknown tokens correctly\n",
    "unknown_text = \"This is an unknownword123\"\n",
    "print(f\"\\nInput: '{unknown_text}'\")\n",
    "encoding = tokenizer.encode(unknown_text)\n",
    "token_ids = encoding.ids\n",
    "tokens = encoding.tokens\n",
    "decoded_text = tokenizer.decode(token_ids, skip_special_tokens=False)\n",
    "print(f\"Token IDs: {token_ids}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Decoded Text: '{decoded_text}'\")\n",
    "\n",
    "# Check specific token IDs for language tokens\n",
    "print(\"\\nVerifying language token IDs:\")\n",
    "for lang in [\"eng\", \"fin\", \"ger\"]:\n",
    "    token_id = tokenizer.token_to_id(f\"[LANG={lang}]\")\n",
    "    print(f\"ID for [LANG={lang}]: {token_id}\")\n",
    "\n",
    "# Check if \"eng\" as a subword is distinct\n",
    "eng_id = tokenizer.token_to_id(\"eng\")\n",
    "print(f\"ID for 'eng' (subword): {eng_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "29db6d6b-bf8d-4e98-944d-89f8ba2d2871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined file saved to train+dev.tsv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def merge_tsv_files(train_file, dev_file, output_file):\n",
    "    # Read the train and dev files as dataframes\n",
    "    train_df = pd.read_csv(\n",
    "    train_file,\n",
    "    sep='\\t',\n",
    "    engine='python',\n",
    "    quoting=3,        # csv.QUOTE_NONE\n",
    "    on_bad_lines='warn',\n",
    "    # If there's no header row, use header=None\n",
    "    # If you do have column names, use header=0\n",
    "    header=0,\n",
    "    encoding='utf-8',)\n",
    "    \n",
    "    dev_df = pd.read_csv(\n",
    "        dev_file,\n",
    "        sep='\\t',\n",
    "        engine='python',\n",
    "        quoting=3,        # csv.QUOTE_NONE\n",
    "        on_bad_lines='warn',\n",
    "        # If there's no header row, use header=None\n",
    "        # If you do have column names, use header=0\n",
    "        header=0,\n",
    "        encoding='utf-8',)\n",
    "        \n",
    "    # Concatenate the two dataframes\n",
    "    combined_df = pd.concat([train_df, dev_df], ignore_index=True)\n",
    "    \n",
    "    # Write the combined dataframe to a new TSV file\n",
    "    combined_df.to_csv(output_file, sep='\\t', index=False)\n",
    "    print(f\"Combined file saved to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Specify your file names here\n",
    "    train_tsv = \"train.tsv\"\n",
    "    dev_tsv = \"dev.tsv\"\n",
    "    output_tsv = \"train+dev.tsv\"\n",
    "    \n",
    "    merge_tsv_files(train_tsv, dev_tsv, output_tsv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32254e90-15b5-49f3-92c8-9e20a8a1ce88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (99000, 3)\n",
      "Dev shape: (13178, 3)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(\n",
    "    \"train.tsv\",\n",
    "    sep='\\t',\n",
    "    engine='python',\n",
    "    quoting=3,        # csv.QUOTE_NONE\n",
    "    on_bad_lines='warn',\n",
    "    # If there's no header row, use header=None\n",
    "    # If you do have column names, use header=0\n",
    "    header=0,\n",
    "    encoding='utf-8',\n",
    ")\n",
    "train_df = pd.read_csv(\n",
    "    \"dev.tsv.tsv\",\n",
    "    sep='\\t',\n",
    "    engine='python',\n",
    "    quoting=3,        # csv.QUOTE_NONE\n",
    "    on_bad_lines='warn',\n",
    "    # If there's no header row, use header=None\n",
    "    # If you do have column names, use header=0\n",
    "    header=0,\n",
    "    encoding='utf-8',\n",
    ")\n",
    "\n",
    "dev_df   = pd.read_csv(\"dev.tsv\", sep='\\t')\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Dev shape:\",   dev_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ef7b66cc-9f31-4d0e-88cc-e12e7c83f350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English rows: 110000\n",
      "German rows:  2000\n",
      "Finnish rows: 200\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def count_languages_in_tsv(filepath):\n",
    "    # Read the TSV\n",
    "    df = pd.read_csv(filepath, sep='\\t')\n",
    "    \n",
    "    # Initialize counters\n",
    "    eng_count = 0\n",
    "    ger_count = 0\n",
    "    fin_count = 0\n",
    "    \n",
    "    # Iterate over the rows\n",
    "    for _, row in df.iterrows():\n",
    "        # Check the 'id' column\n",
    "        row_id = str(row['id'])  # Make sure it's a string\n",
    "        if row_id.startswith('eng_'):\n",
    "            eng_count += 1\n",
    "        elif row_id.startswith('ger_'):\n",
    "            ger_count += 1\n",
    "        elif row_id.startswith('fin_'):\n",
    "            fin_count += 1\n",
    "    \n",
    "    # Print or return the results\n",
    "    print(f\"English rows: {eng_count}\")\n",
    "    print(f\"German rows:  {ger_count}\")\n",
    "    print(f\"Finnish rows: {fin_count}\")\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    tsv_file = \"new_train.tsv\"\n",
    "    count_languages_in_tsv(tsv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "55180de6-f439-4715-b8ad-dd72fad4ddfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 78540 rows to new_train.tsv\n",
      "Saved 33660 rows to new_dev.tsv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def split_multilingual_data(input_file, output_train, output_dev, train_frac=0.7, random_state=42):\n",
    "    # Read the TSV into a DataFrame\n",
    "    df = pd.read_csv(input_file, sep='\\t')\n",
    "    \n",
    "    # Separate rows by language\n",
    "    df_eng = df[df['id'].str.startswith('eng_')]\n",
    "    df_ger = df[df['id'].str.startswith('ger_')]\n",
    "    df_fin = df[df['id'].str.startswith('fin_')]\n",
    "    \n",
    "    # Shuffle & split each language\n",
    "    # English\n",
    "    eng_train = df_eng.sample(frac=train_frac, random_state=random_state)\n",
    "    eng_dev   = df_eng.drop(eng_train.index)\n",
    "    \n",
    "    # German\n",
    "    ger_train = df_ger.sample(frac=train_frac, random_state=random_state)\n",
    "    ger_dev   = df_ger.drop(ger_train.index)\n",
    "    \n",
    "    # Finnish\n",
    "    fin_train = df_fin.sample(frac=train_frac, random_state=random_state)\n",
    "    fin_dev   = df_fin.drop(fin_train.index)\n",
    "    \n",
    "    # Combine splits across languages\n",
    "    train_df = pd.concat([eng_train, ger_train, fin_train], ignore_index=True)\n",
    "    dev_df   = pd.concat([eng_dev,   ger_dev,   fin_dev],   ignore_index=True)\n",
    "    \n",
    "    # Shuffle again (optional, to mix languages in the final output)\n",
    "    train_df = train_df.sample(frac=1.0, random_state=random_state).reset_index(drop=True)\n",
    "    dev_df   = dev_df.sample(frac=1.0, random_state=random_state).reset_index(drop=True)\n",
    "    \n",
    "    # Save to new TSV files\n",
    "    train_df.to_csv(output_train, sep='\\t', index=False)\n",
    "    dev_df.to_csv(output_dev, sep='\\t', index=False)\n",
    "    \n",
    "    # Print some stats\n",
    "    print(f\"Saved {len(train_df)} rows to {output_train}\")\n",
    "    print(f\"Saved {len(dev_df)} rows to {output_dev}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_tsv = \"new_combined.tsv\"       # Your combined file\n",
    "    output_train_tsv = \"new_train.tsv\" # 70% split\n",
    "    output_dev_tsv   = \"new_dev.tsv\"   # 30% split\n",
    "    \n",
    "    split_multilingual_data(input_tsv, output_train_tsv, output_dev_tsv, train_frac=0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b5ff7832-40c4-4955-805d-9eb73d7b28a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English:\n",
      "  Toxic (label=1):     40349\n",
      "  Non-toxic (label=0): 69651\n",
      "\n",
      "German:\n",
      "  Toxic (label=1):     500\n",
      "  Non-toxic (label=0): 1500\n",
      "\n",
      "Finnish:\n",
      "  Toxic (label=1):     150\n",
      "  Non-toxic (label=0): 50\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def count_toxic_by_language(filepath):\n",
    "    # Read the TSV into a DataFrame\n",
    "    df = pd.read_csv(filepath, sep='\\t')\n",
    "    \n",
    "    # Separate by language using 'id' prefix\n",
    "    df_eng = df[df['id'].str.startswith('eng_')]\n",
    "    df_ger = df[df['id'].str.startswith('ger_')]\n",
    "    df_fin = df[df['id'].str.startswith('fin_')]\n",
    "    \n",
    "    # Function to count 0/1 labels\n",
    "    def label_counts(sub_df):\n",
    "        # Count how many label=0 and how many label=1\n",
    "        toxic_count = len(sub_df[sub_df['label'] == 1])\n",
    "        non_toxic_count = len(sub_df[sub_df['label'] == 0])\n",
    "        return toxic_count, non_toxic_count\n",
    "    \n",
    "    # Count for each language\n",
    "    eng_toxic, eng_non_toxic = label_counts(df_eng)\n",
    "    ger_toxic, ger_non_toxic = label_counts(df_ger)\n",
    "    fin_toxic, fin_non_toxic = label_counts(df_fin)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"English:\")\n",
    "    print(f\"  Toxic (label=1):     {eng_toxic}\")\n",
    "    print(f\"  Non-toxic (label=0): {eng_non_toxic}\")\n",
    "    print()\n",
    "    print(\"German:\")\n",
    "    print(f\"  Toxic (label=1):     {ger_toxic}\")\n",
    "    print(f\"  Non-toxic (label=0): {ger_non_toxic}\")\n",
    "    print()\n",
    "    print(\"Finnish:\")\n",
    "    print(f\"  Toxic (label=1):     {fin_toxic}\")\n",
    "    print(f\"  Non-toxic (label=0): {fin_non_toxic}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tsv_file = \"new_combined.tsv\"\n",
    "    count_toxic_by_language(tsv_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6f7222f6-838f-4b78-a946-bfa2808954bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English:\n",
      "  Toxic (label=1):     28220\n",
      "  Non-toxic (label=0): 48780\n",
      "\n",
      "German:\n",
      "  Toxic (label=1):     341\n",
      "  Non-toxic (label=0): 1059\n",
      "\n",
      "Finnish:\n",
      "  Toxic (label=1):     105\n",
      "  Non-toxic (label=0): 35\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def count_toxic_by_language(filepath):\n",
    "    # Read the TSV into a DataFrame\n",
    "    df = pd.read_csv(filepath, sep='\\t')\n",
    "    \n",
    "    # Separate by language using 'id' prefix\n",
    "    df_eng = df[df['id'].str.startswith('eng_')]\n",
    "    df_ger = df[df['id'].str.startswith('ger_')]\n",
    "    df_fin = df[df['id'].str.startswith('fin_')]\n",
    "    \n",
    "    # Function to count 0/1 labels\n",
    "    def label_counts(sub_df):\n",
    "        # Count how many label=0 and how many label=1\n",
    "        toxic_count = len(sub_df[sub_df['label'] == 1])\n",
    "        non_toxic_count = len(sub_df[sub_df['label'] == 0])\n",
    "        return toxic_count, non_toxic_count\n",
    "    \n",
    "    # Count for each language\n",
    "    eng_toxic, eng_non_toxic = label_counts(df_eng)\n",
    "    ger_toxic, ger_non_toxic = label_counts(df_ger)\n",
    "    fin_toxic, fin_non_toxic = label_counts(df_fin)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"English:\")\n",
    "    print(f\"  Toxic (label=1):     {eng_toxic}\")\n",
    "    print(f\"  Non-toxic (label=0): {eng_non_toxic}\")\n",
    "    print()\n",
    "    print(\"German:\")\n",
    "    print(f\"  Toxic (label=1):     {ger_toxic}\")\n",
    "    print(f\"  Non-toxic (label=0): {ger_non_toxic}\")\n",
    "    print()\n",
    "    print(\"Finnish:\")\n",
    "    print(f\"  Toxic (label=1):     {fin_toxic}\")\n",
    "    print(f\"  Non-toxic (label=0): {fin_non_toxic}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tsv_file = \"new_train.tsv\"\n",
    "    count_toxic_by_language(tsv_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8b2383-64eb-42de-a116-34d0c568dda3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
