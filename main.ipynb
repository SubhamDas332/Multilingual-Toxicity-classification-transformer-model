{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "JEIMpFltu7ke"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "from tokenizers.processors import BertProcessing\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "QpL3me5Mu7kf"
   },
   "outputs": [],
   "source": [
    "class SimpleBPETokenizer:\n",
    "    def __init__(self, vocab_size=30_000):\n",
    "        self.tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "        self.tokenizer.pre_tokenizer = ByteLevel()\n",
    "        self.trainer = BpeTrainer(vocab_size=vocab_size, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\",\"[LANG=eng]\", \"[LANG=fin]\", \"[LANG=ger]\"])\n",
    "\n",
    "    def train(self, files):\n",
    "        self.tokenizer.train(files, self.trainer)\n",
    "\n",
    "    def save(self, path):\n",
    "        self.tokenizer.save(path)\n",
    "\n",
    "    def load(self, path):\n",
    "        self.tokenizer = Tokenizer.from_file(path)\n",
    "\n",
    "    def encode(self, text):\n",
    "        return self.tokenizer.encode(text).tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        return self.tokenizer.decode(tokens)\n",
    "\n",
    "    def encode_ids(self, text):\n",
    "        return self.tokenizer.encode(text).ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "v_52qe9Eu7kg"
   },
   "outputs": [],
   "source": [
    "tokenizer = SimpleBPETokenizer()\n",
    "# tokenizer.train([\"language_tokenizer_corpus.txt\"])\n",
    "# tokenizer.save(\"lang_bpe_tokenizer.json\")\n",
    "tokenizer.load(\"lang_bpe_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "xK5lHbgEu7kg"
   },
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * heads == embed_size\n",
    "        ), \"Embedding size must be divisible by number of heads\"\n",
    "\n",
    "        self.query = nn.Linear(embed_size, embed_size)\n",
    "        self.key = nn.Linear(embed_size, embed_size)\n",
    "        self.value = nn.Linear(embed_size, embed_size)\n",
    "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        N, seq_length, embed_size = x.shape\n",
    "        Q = self.query(x).view(N, seq_length, self.heads, self.head_dim).transpose(1, 2)\n",
    "        K = self.key(x).view(N, seq_length, self.heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.value(x).view(N, seq_length, self.heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "\n",
    "        if mask is not None:\n",
    "            attention_scores = attention_scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "\n",
    "        attention = torch.softmax(attention_scores, dim=-1)\n",
    "        out = torch.matmul(attention, V).transpose(1, 2).contiguous().view(N, seq_length, embed_size)\n",
    "\n",
    "        return self.fc_out(out)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, forward_expansion, dropout):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadSelfAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * embed_size, embed_size),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_output = self.attention(x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        forward_out = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(forward_out))\n",
    "        return x\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_layers, heads, forward_expansion, dropout, max_length):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [TransformerBlock(embed_size, heads, forward_expansion, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc_out = nn.Linear(embed_size, 2)  # Output 2 classes (toxic or non-toxic)\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(x.device)\n",
    "        out = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n",
    "\n",
    "        # Create attention mask (1s for real tokens, 0s for padding)\n",
    "        mask = (x != 0).unsqueeze(1).unsqueeze(2)  # Shape: (N, 1, 1, seq_length)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, mask)  # Pass mask to each TransformerBlock\n",
    "\n",
    "        return self.fc_out(out[:, 0, :])  # CLS token for classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "vprFj1m0u7kh"
   },
   "outputs": [],
   "source": [
    "class ToxicDataset(Dataset):\n",
    "    def __init__(self, tsv_file, tokenizer, max_length=128):\n",
    "        # Load the TSV file\n",
    "        df = pd.read_csv(tsv_file, sep=\"\\t\")\n",
    "        self.ids = df[\"id\"].values  # Load ids containing language info\n",
    "        self.texts = df[\"text\"].values\n",
    "        self.labels = df[\"label\"].values  # Labels present in train/dev\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        id_ = self.ids[idx]\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Extract language from id (e.g., 'eng' from 'eng_train0')\n",
    "        lang = id_.split('_')[0]\n",
    "        lang_token = f\"[LANG={lang}]\"\n",
    "        text_with_lang = lang_token + \" \" + text  # Prepend language token\n",
    "\n",
    "        # Tokenize the text with language token\n",
    "        encoded = self.tokenizer.tokenizer.encode(text_with_lang)\n",
    "        cls_id = self.tokenizer.tokenizer.token_to_id(\"[CLS]\")\n",
    "        sep_id = self.tokenizer.tokenizer.token_to_id(\"[SEP]\")\n",
    "        input_ids = [cls_id] + encoded.ids[:self.max_length - 2] + [sep_id]\n",
    "\n",
    "        # Pad to max_length\n",
    "        padding_len = self.max_length - len(input_ids)\n",
    "        input_ids += [0] * padding_len\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long),\n",
    "            \"lang\": lang  # Add language to the batch\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "3z8W3RDou7kh"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer, criterion, device,class_weights):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        langs = batch[\"lang\"]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Enable autocast for mixed precision\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            outputs = model(input_ids)\n",
    "            losses = criterion(outputs, labels)\n",
    "\n",
    "            sample_weights = torch.zeros_like(losses)\n",
    "            for i, (lang, label) in enumerate(zip(langs, labels)):\n",
    "                sample_weights[i] = class_weights[lang][label.item()]\n",
    "            loss = (losses * sample_weights).mean()\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # Rest of the code remains the same...\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device,class_weights):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    all_labels, all_preds = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "            langs = batch[\"lang\"]\n",
    "\n",
    "            outputs = model(input_ids)\n",
    "            losses = criterion(outputs, labels)\n",
    "\n",
    "            sample_weights = torch.zeros_like(losses)\n",
    "            for i, (lang, label) in enumerate(zip(langs, labels)):\n",
    "                sample_weights[i] = class_weights[lang][label.item()]\n",
    "            loss = (losses * sample_weights).mean()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / total\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    return avg_loss, accuracy, f1\n",
    "\n",
    "def train_model(model, train_loader, dev_loader, optimizer, criterion, device,class_weights, epochs):\n",
    "    best_val_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch [{epoch+1}/{epochs}]\")\n",
    "\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device,class_weights)\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "\n",
    "        val_loss, val_acc , f1= evaluate(model, dev_loader, criterion, device,class_weights)\n",
    "        print(f\"Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f} | f1:   {f1:.4f}\")\n",
    "        \n",
    "        # Save the best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), f\"{epoch+1}_{val_loss:.4f}_best_transformer_model.pth\")\n",
    "            print(\"ðŸ”¹ Model Saved (best so far)!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pxz0c1Zcu7ki",
    "outputId": "1408489b-9d81-4d7e-8a27-cb65615c2c83"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerEncoder(\n",
       "  (word_embedding): Embedding(30000, 512)\n",
       "  (position_embedding): Embedding(256, 512)\n",
       "  (layers): ModuleList(\n",
       "    (0-7): 8 x TransformerBlock(\n",
       "      (attention): MultiHeadSelfAttention(\n",
       "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_forward): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=3072, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=3072, out_features=512, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.3, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (fc_out): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# In the hyperparameters cell (modify these values):\n",
    "VOCAB_SIZE = 30000\n",
    "EMBED_SIZE = 512\n",
    "NUM_LAYERS = 8\n",
    "HEADS = 8\n",
    "FORWARD_EXPANSION = 6\n",
    "DROPOUT = 0.3\n",
    "MAX_LENGTH = 256\n",
    "LR = 1e-4\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 128\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Optimize DataLoader configuration\n",
    "NUM_WORKERS = 8\n",
    "PIN_MEMORY = True\n",
    "PREFETCH_FACTOR = 4\n",
    "PERSISTENT_WORKERS = None\n",
    "\n",
    "class_weights = {\n",
    "    'eng': torch.tensor([0.79, 1.36], device=DEVICE),  # [Non-toxic, Toxic]\n",
    "    'ger': torch.tensor([0.66, 2.05], device=DEVICE),\n",
    "    'fin': torch.tensor([2.00, 0.67], device=DEVICE)\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "model = TransformerEncoder(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embed_size=EMBED_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    heads=HEADS,\n",
    "    forward_expansion=FORWARD_EXPANSION,\n",
    "    dropout=DROPOUT,\n",
    "    max_length=MAX_LENGTH\n",
    ").to(DEVICE)\n",
    "\n",
    "# Define optimizer and loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "model.share_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ySAUxO02u7kj"
   },
   "outputs": [],
   "source": [
    "# Create train & dev Datasets\n",
    "train_dataset = ToxicDataset(\"new_train.tsv\", tokenizer, max_length=MAX_LENGTH)\n",
    "dev_dataset = ToxicDataset(\"new_dev.tsv\", tokenizer, max_length=MAX_LENGTH)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    prefetch_factor=PREFETCH_FACTOR\n",
    ")\n",
    "\n",
    "dev_loader = DataLoader(\n",
    "    dev_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    prefetch_factor=PREFETCH_FACTOR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QqNW2DAcu7kj",
    "outputId": "f44bf0bf-532a-49da-f7df-96740f9eb37d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [1/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6376 | Train Acc: 0.6200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss:   0.4776 | Val Acc:   0.7924 | f1:   0.7888\n",
      "ðŸ”¹ Model Saved (best so far)!\n",
      "\n",
      "Epoch [2/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4659 | Train Acc: 0.7862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss:   0.4269 | Val Acc:   0.8398 | f1:   0.8353\n",
      "ðŸ”¹ Model Saved (best so far)!\n",
      "\n",
      "Epoch [3/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4043 | Train Acc: 0.8246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss:   0.3730 | Val Acc:   0.8608 | f1:   0.8586\n",
      "ðŸ”¹ Model Saved (best so far)!\n",
      "\n",
      "Epoch [4/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3637 | Train Acc: 0.8495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss:   0.3421 | Val Acc:   0.8737 | f1:   0.8725\n",
      "ðŸ”¹ Model Saved (best so far)!\n",
      "\n",
      "Epoch [5/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3315 | Train Acc: 0.8650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss:   0.3328 | Val Acc:   0.8848 | f1:   0.8836\n",
      "ðŸ”¹ Model Saved (best so far)!\n",
      "\n",
      "Epoch [6/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3078 | Train Acc: 0.8764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss:   0.3158 | Val Acc:   0.8887 | f1:   0.8875\n",
      "ðŸ”¹ Model Saved (best so far)!\n",
      "\n",
      "Epoch [7/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2857 | Train Acc: 0.8877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss:   0.3139 | Val Acc:   0.8975 | f1:   0.8967\n",
      "ðŸ”¹ Model Saved (best so far)!\n",
      "\n",
      "Epoch [8/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2634 | Train Acc: 0.8961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss:   0.3166 | Val Acc:   0.8982 | f1:   0.8983\n",
      "\n",
      "Epoch [9/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2453 | Train Acc: 0.9040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss:   0.3149 | Val Acc:   0.9035 | f1:   0.9029\n",
      "\n",
      "Epoch [10/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2293 | Train Acc: 0.9110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss:   0.3064 | Val Acc:   0.9033 | f1:   0.9027\n",
      "ðŸ”¹ Model Saved (best so far)!\n",
      "\n",
      "Epoch [11/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2164 | Train Acc: 0.9152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss:   0.2956 | Val Acc:   0.9067 | f1:   0.9065\n",
      "ðŸ”¹ Model Saved (best so far)!\n",
      "\n",
      "Epoch [12/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2052 | Train Acc: 0.9211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss:   0.3129 | Val Acc:   0.9048 | f1:   0.9045\n",
      "\n",
      "Epoch [13/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1912 | Train Acc: 0.9253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss:   0.3312 | Val Acc:   0.9047 | f1:   0.9047\n",
      "\n",
      "Epoch [14/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1816 | Train Acc: 0.9292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss:   0.2991 | Val Acc:   0.9058 | f1:   0.9058\n",
      "\n",
      "Epoch [15/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1705 | Train Acc: 0.9338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss:   0.3008 | Val Acc:   0.9072 | f1:   0.9074\n",
      "\n",
      "Epoch [16/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1597 | Train Acc: 0.9376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss:   0.3244 | Val Acc:   0.9075 | f1:   0.9075\n",
      "\n",
      "Epoch [17/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1520 | Train Acc: 0.9413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss:   0.3046 | Val Acc:   0.9053 | f1:   0.9058\n",
      "\n",
      "Epoch [18/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1436 | Train Acc: 0.9444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss:   0.3883 | Val Acc:   0.9087 | f1:   0.9082\n",
      "\n",
      "Epoch [19/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mset_device(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      2\u001b[0m torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcudnn\u001b[38;5;241m.\u001b[39mbenchmark \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# Speeds up convolutions (if any)\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdev_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdev_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 75\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, dev_loader, optimizer, criterion, device, class_weights, epochs)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43mclass_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Train Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     78\u001b[0m     val_loss, val_acc , f1\u001b[38;5;241m=\u001b[39m evaluate(model, dev_loader, criterion, device,class_weights)\n",
      "Cell \u001b[0;32mIn[10], line 24\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, dataloader, optimizer, criterion, device, class_weights)\u001b[0m\n\u001b[1;32m     21\u001b[0m     loss \u001b[38;5;241m=\u001b[39m (losses \u001b[38;5;241m*\u001b[39m sample_weights)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     23\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 24\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Rest of the code remains the same...\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/env-name/lib/python3.9/site-packages/torch/amp/grad_scaler.py:457\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    455\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 457\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/.conda/envs/env-name/lib/python3.9/site-packages/torch/amp/grad_scaler.py:351\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    345\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    349\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    350\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf_per_device\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    352\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/.conda/envs/env-name/lib/python3.9/site-packages/torch/amp/grad_scaler.py:351\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    345\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    349\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    350\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    352\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.cuda.set_device(0)\n",
    "torch.backends.cudnn.benchmark = True  # Speeds up convolutions (if any)\n",
    "\n",
    "train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    dev_loader=dev_loader,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    device=DEVICE,\n",
    "    class_weights=class_weights,\n",
    "    epochs=EPOCHS\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1XxvoKq8u7kj",
    "outputId": "40a257eb-f039-4efd-9081-7ffac123593c"
   },
   "outputs": [],
   "source": [
    "\n",
    "# torch.save(model.state_dict(), \"best_transformer_model.pth\")\n",
    "# print(\"ðŸ”¹ Model Saved (best so far)!\")\n",
    "model.load_state_dict(torch.load(\"9_0.2803_best_transformer_model.pth\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 49107970\n"
     ]
    }
   ],
   "source": [
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {num_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3kZ8PwWlu7kk"
   },
   "outputs": [],
   "source": [
    "def classify_text(text, lang, tokenizer, model, device, max_length=256):\n",
    "    lang_token = f\"[LANG={lang}]\"\n",
    "    text_with_lang = lang_token + \" \" + text\n",
    "    encoded = tokenizer.tokenizer.encode(text_with_lang)\n",
    "    cls_id = tokenizer.tokenizer.token_to_id(\"[CLS]\")\n",
    "    sep_id = tokenizer.tokenizer.token_to_id(\"[SEP]\")\n",
    "    input_ids = [cls_id] + encoded.ids[:max_length - 2] + [sep_id]\n",
    "    padding_len = max_length - len(input_ids)\n",
    "    input_ids += [0] * padding_len\n",
    "    input_ids = torch.tensor([input_ids], dtype=torch.long).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "    return preds.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"nein deutsh\"\n",
    "print(tokenizer.encode(sample_text))  # Check if subwords make sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "60_lOa3_u7kk",
    "outputId": "4684d18b-b688-4349-c8ed-eabd04de9685"
   },
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test_df = pd.read_csv('test.tsv', sep='\\t', header=0, quoting=3)\n",
    "\n",
    "# Classify each text by extracting language from id\n",
    "test_df[\"predicted\"] = test_df.apply(\n",
    "    lambda row: classify_text(row[\"text\"], row[\"id\"].split('_')[0], tokenizer, model, DEVICE),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Drop unnecessary columns, keeping 'id' and 'predicted'\n",
    "columns_to_drop = ['text']\n",
    "if 'Unnamed: 0' in test_df.columns:\n",
    "    columns_to_drop.append('Unnamed: 0')\n",
    "test_df = test_df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Save predictions\n",
    "test_df.to_csv(\"predictions.tsv\", sep=\"\\t\", index=False)\n",
    "print(\"Final predictions saved to 'predictions.tsv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xG-UboX_u7kk",
    "outputId": "62512d7c-bca6-4495-8b28-8b4eb3881683"
   },
   "outputs": [],
   "source": [
    "print(pd.Series(dev_dataset.labels).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ViUzXvytu7kk",
    "outputId": "2abc7d47-0b83-45df-972c-bab407ca62af"
   },
   "outputs": [],
   "source": [
    "print(pd.Series(train_dataset.labels).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fMQSdPknu7kl",
    "outputId": "131e7cdf-f37a-4509-ad89-5f20b0b9620d"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1 = f1_score(labels.cpu(), preds.cpu(), average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Zx-496Yu7kl",
    "outputId": "ba88a894-964b-4d0c-e68d-d068da331612"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the TSV file\n",
    "try:\n",
    "    df = pd.read_csv(\"dev.tsv\", sep=\"\\t\", encoding=\"utf-8\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'dev.tsv' file not found. Please ensure the file is in the same directory as this script.\")\n",
    "    exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading the file: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Inspect the columns to confirm the structure\n",
    "print(\"Columns in dev.tsv:\", df.columns.tolist())\n",
    "\n",
    "# Define column names based on the observed structure\n",
    "id_column = \"id\"  # Column containing the ID (e.g., fin_dev_0, eng_dev_1)\n",
    "text_column = \"text\"  # Column containing the text\n",
    "label_column = \"label\"  # Column with toxicity labels (1 or 0)\n",
    "\n",
    "# Verify required columns exist\n",
    "required_columns = [id_column, text_column, label_column]\n",
    "missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "if missing_columns:\n",
    "    print(f\"Error: Missing required columns: {missing_columns}\")\n",
    "    print(\"Please check the column names in dev.tsv and adjust the script accordingly.\")\n",
    "    exit(1)\n",
    "\n",
    "# Filter rows where 'fin_' exists in the id column (case-insensitive)\n",
    "df[\"is_finnish\"] = df[id_column].str.contains(\"fin_\", case=False, na=False)\n",
    "finnish_texts = df[df[\"is_finnish\"]]\n",
    "\n",
    "# Check if there are any Finnish texts\n",
    "if finnish_texts.empty:\n",
    "    print(\"No Finnish texts found in dev.tsv (no 'fin_' in id column).\")\n",
    "    exit(0)\n",
    "\n",
    "# Count toxic (1) and non-toxic (0) Finnish texts\n",
    "toxic_count = finnish_texts[finnish_texts[label_column] == 1].shape[0]\n",
    "non_toxic_count = finnish_texts[finnish_texts[label_column] == 0].shape[0]\n",
    "\n",
    "# Print the results\n",
    "print(\"\\nResults:\")\n",
    "print(f\"Number of Finnish toxic texts (label=1): {toxic_count}\")\n",
    "print(f\"Number of Finnish non-toxic texts (label=0): {non_toxic_count}\")\n",
    "print(f\"Total Finnish texts: {toxic_count + non_toxic_count}\")\n",
    "\n",
    "# Optional: Display a few examples\n",
    "print(\"\\nSample Finnish Texts:\")\n",
    "for idx, row in finnish_texts.head(5).iterrows():\n",
    "    print(f\"ID: {row[id_column]}\")\n",
    "    print(f\"Text: {row[text_column]}\")\n",
    "    print(f\"Label: {row[label_column]}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y1pH8OvkGX1M"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
